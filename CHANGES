
Friday May 20th, 2016
=====================

    Sketched out a new article just on the crypto approach + sequence 64bit nonce.

    Seems appropriate. There is enough detail in here just to cover encryption before going on to secure server.

    Fuck, the hash isn't even a real hash. It's a fucking set! What the fuck bitsquid guys?!

    The key value are the same. It's a set. A fucking set not a hash!

    I need an actual hash with key -> value mapping.

    Fuck it, just work around it with something O(n) for the mapping.

    Good enough for now, and can be replaced with something more awesome later. Get it done!

    Hacked up a really simple data structure. Not implement

    Add counters to print out at the end of the packet encryption test

    Make sure everything is working OK

    Seems fine.

    Fix the salt timeout it's broken

    Search through additional todos. Seems fine. Added some more notes.

    Specifically, I want a way to genericalyl abort packet read at the TYPE level, if that type is not allowed for some reason.

    eg. a type that the server doesn't expect, *or* an unencrypted packet for a type that should be encrypted and so on.

    This part is probably important to do before saying the packet encryption part is finished, because that's a key part of being secure.

    Don't let the client just get in under the covers with an unencrypted packet!

    Doing this now...

    OK. This is pretty clean. I like the PacketInfo approach for extending read and write packets.

    Get the code compiling on windows (install sodium)

    Wow. It's totally fucked up again. How?!

    First step, disable sodium by implementing #define YOJIBO_SECURE

    If this is not defined, you can strip out all the security support.

    Got the security stripped out and got everything working on Mac and Linux.

    PRIx64 WTF. gross...

    Now back to windows.

    Repaired the Visual Studio installation. Really I don't have any other explanation for why it is busted other than corruption.

    I don't know what is going on here.

    WINDOWS

    It's fucked up because I included the sockets implementation, which probably needs more headers.

    Without the sockets class it's fine. What else needs to be included for sockets?

    Actually, it was some weird ass s_addr #define shit. Ugh. Fixed.

    Now turn back on YOJIMBO_SECURE and get it working under linux.

    Done. Had to make from source and then run "ldconfig" to update shared libraries after installing.

    Now bring it back into windows and get sodium working and installing there.

    Package up something for supporters this month. It should make them happy, but be low key about it.

    Kinda brought sodium up and working in windows. Bundling it seems to make the most sense.

    Some errors with I think uint8_t pointer access through my uint64_t for the sequence #

    Hopefully this can be fixed, windows is complaining about it but mac and linux working fine.

    Should add packet encrypt/decrypt tests as well as AEAD tests to tests.cpp #if HAS( YOJIMBO_SECURE )
    
    No dice. There is nothing wrong with the sequence number compress/decompress.

    I'm doing something wrong, either undefined behavior (three different compilers bitching about something = MY FAULT)

    or memory trash

    or stack trash or something.

    Not sure yet!

    Have to find it. Can't put this code out there without this.

    With the simplified code, the decrypts work in windows and mac, but now fail on linux.

    Could it be that linux does not like:

        a) unaligned decrypt

        b) decrypt in place?

    Lets try some experiments...

    Seems like linux just doesn't want to decrypt at all. Broken libsodium?

    Add a test for sodium packet encrypt/decrypt in lib sodium.

    Passes. I now suspect stack trashing and/or memory trashing.

    Try not using the stupid yojimbo allocator. See if that fixes it.

    Long shot. Didn't seem to help at all.

    I really don't know what is going on.

    This is fucked. =p

    By taking a block that fails to decrypt on the server, and inputting it, it also fails to decrypt on mac.

    This indicates to me that something is going wrong in the ENCRYPT level. eg. it is encrypting to crap, for some reason,
    and this encrypted block cannot be decrypted because it it trashed.

    Idea: Pass in known packet bytes, eg. packet of size 1 with contents [ 1 ] and see what it encrypts to on linux vs. on mac.

    OK. I've actually fucking caught it doing stupid crap in linux now. I have a small trivial test where on linux it fails,
    and gives random encrypted packets, while no macosx it hits a proper fixed encoding.

    I think this isolates the bug on linux. 

    I also don't think it's mine.

    You know what would fit. If the simple box nonce was larger than the bytes specified, or the key.

    Yeah that would certainly explain what I'm seeing...

    Found it. The key and nonce sizes are different for the simplebox.

    Fuck me. That was it.

    Both windows and mac working perfectly.

    Linux is now complaining about serialize checks failing

    Truncation?

    Logic error with the sequence compression?

    WTF... need to dig in. Tomorrow.


Thursday May 19th, 2016
=======================

    No bugs says the nonce must *never* repeat

    So in fact, it's better for it to not be a randomly generated 64bit number, but for it to be a 64bit number that increases is much better.

    This can be done by adding the 64 bit sequence # to the end of the packet, it should never wrap around.

    64bit sequence can be encoded better by prefixing it with the # of zero bytes

    This means the token has no need to send nonce, but it does need two private keys, one for each direction.

    Now I need a way to actually send the packet through with the nonce, the packet needs to know its sequence # to encrpypt.

    Added uint64_t sequence to send and receive packets, and to the packet entry. This way I can append this sequence at the end of the packet data.

    Packet structure looks something like this:

    [encrypted 0/1, bitfield for compressing the 64bit sequence, eg. 7 high bits, are they non-zero]

    [packet data, possibly encrypted]

    [auth 8 bytes]

    [1-8 bytes of sequence data]

    Sorted! This is a plan!

    ps. if packet is NOT encrypted, just make that whole first byte ZERO.

    Finally finished the encoding of the variable length uint64_t sequence #

    Added code to stick the top bit to 1 in encrypted packets, and check this on packet read to know what to do.

    Now to hook up the packet to encode the sequence # in the prefix byte and the end of the packet.

    Had to disable stream check that no bits are left in packet to serialize for this to work.

    Verify this writes and reads back properly. Only do it for encrypted mode.

    Once this passes, extend to actually perform the encryption, eg. copy across to another buffer.

    Having some weird desync with the decryption refusing to work.

    Why don't I just have a variable # of prefix bytes. It so much simple than what I'm currently doing.

    Halfway done. This would also allow me to simplify the compress/decompress sequence.

    Spent all the night tracking down what appears to be a compiler bug, memory trash or logic error where m_key was trashed.

    Now bring back all the complex stuff, holding nonce/key fixed.

    Once working with fixed nonce/key. STOP and then investigate if memory trash is occuring.


Wednesday May 18th, 2016
========================

    Start playing around with libsodium and encrypted packets.

    Sketch out token structure, connection request packet, generate tokens etc.

    Implement token encrypt.

    Implement token decrypt.

    Implement token comparison.

    Verify decrypted token matches original token.

    Passes!

    Overhead for encryption per-packet = 16 bytes.

    Now implement packet encryption and decryption.

    Given a packet of a given size, knowing the key and nonce, encrypt it.

    Next, given encrypted packet of given size, knowing key and nonce, decrypt it.

    Presumably extra data needs to be added to the end of the packet?

    Once this is done, I have all the building blocks I need for the secure server!    

    Now to actually design the packet format for the encryption packet

    I think there needs to be an initial byte that tells you how to handle the packet

    The initial byte needs to have the packet type, no question.

    Because certain packet types will be encrypted and others will not be.

    The packet factory needs to know which packets are encrypted on a per-type basis.

    Protocol2 should be able to build without security if it is desired, so #if PROTOCOL_SECURE is necessary.

    To encrypt a packet, you need to know:

        a) packet type
        b) packet data
        c) nonce
        d) key

    To decrypt a packet, you need to know:

        a) packet type (some types may not be compressed, eg. connection request)
        b) address it comes from
        c) some mapping from address to (nonce,key) pair for that address

    Does not seem that the encryption can be done at the protocol2 level entirely,
    because it requires a mapping from address <-> (nonce,key), and address lives in network2.

    Seems that it must push mostly up to yojimbo level.

    But still I think the encrypted per-packet is in the protocol2 level.

    No. Per packet encryption and everything has been moved into yojimbo. Leaving protocol2 alone.

    Seems that the encrypted packet work needs to be done there, because it must allocate
    data structures, and I don't want to have to move the custom allocator support down
    into protocol2.

    ---------

    I think the packet header stuff is a bit shitty.

    Is it really necessary? I mean you'd have to queue up the packet headers in addition to the packet types.

    What need do I have for a packet header separate from the packet types?

    Also, it makes encryption really annoying, because the header would need to go after the type.

    Packet header is used in packet aggregation and packet fragmentation and reassembly examples.

    Can these examples be implemented without this header support?

    It would be nice if it could, because it would be much simpler.

    IMO. A customized header shoud just be incorporated into a base packet type! *DONE*
    
    ------

    Need to think long and hard about the header.

    On one hand it is very flexible.

    On the other, it's a huge pain in the ass for integration into a network interface.

    I feel like what I really want is a simple one byte value before each packet, eg. packet flags:

        fragment, encrypted, etc.

    This way fragment/encrypted don't have a dependency on 'type' which doesn't makes sense anyway for an aggregate packet.

    Seems like the decision to encrypt or not when sending a packet should be allowed on a per-packet basis,
    but of course, the sender should only accept certain types of packets when encrypted, and discard any
    non-encrypted ones sent to them.

    So really, you only even want to allow the 'connection request' to be unencrypted and everything else must be encrypted,
    including fragments, aggregate packets and so on.

    -------

    Idea: flag per-packet, PACKET_FLAG_DO_NOT_AGGREGATE, PACKET_FLAG_ENCRYPT, PACKET_FLAG_FRAGMENT

    Perhaps I should just gut protocol2 and move it into yojimbo and rethink.

    -------

    We do actually need the packet header system so we can put a sequence # on each packet

    That doesn't mean this sequence number needs to actually get passed in through to the user.

    But it is necessary to implement the packet fragmentation and reassembly example, because
    you need to know the sequence # of a packet if you are to try to reassemble it from fragments.

    Seems like the packet header concept is here to stay.

    I think the packet header doesn't belong to the user, the packet header belongs to the *INTERFACE*

    Therefore, the interface can put whatever it wants in there, and its stripped before it gets to the user (Packet*)

    This way the packet header is a flexible tool. It should stay. Just don't ever expose packet headers out of the 
    interface layer to the user. Don't put them in send and receive queues etc.

    -------

    So to get the encryption working, there needs to be a byte inserted in front of all packets with a flag,
    eg. encrypted yes/no.

    On send:

        the sender can check the packet type, if its encrypted, send it encrypted (get nonce/key pair from address map, *fail* if not found)

    On receive:

        the receiver checks the flag to see if encrypted. Type is *inside* the encrypted bit. 

        if not encrypted, check type. if type is encrypted, but packet is not, discard. otherwise process.

        if encrypted, search for nonce/key pair for from address. if doesn't exist, throw away. otherwise, decrypt.

        check type. if type is encrypted, allow packet through. if type is decrypted, discard packet (strict)

    I think this means I need to in the SocketInterface level stick a one byte for flags in front of each packet,
    and in each packet entry in the queues. This is actually a neat solution, because I can put cool flags in there:

        PACKET_FLAG_DONT_AGGREGATE
        PACKET_FLAG_FRAGMENT          // *do not expose this to the user*, internal use only.
        PACKET_FLAG_ENCRYPTED
        PACKET_FLAG_COMPRESSED        // if some form of compression is done

    This is very flexible, and a byte should be plenty!

    -------------

    Seems like the packet flags should be part of the additional data

    eg. you would not like somebody to be able to modify them, but you don't want them encrypted, you just want them signed.

    Seems that I should use AEAD for the packet payloads too!

    -------------

    Suggested packet format at this point in time:

        [packet_flags][encrypted_packet_data][auth]

    -------------

    Add packet flags to yojimbo

    In write packet, pass in rawFormat = true

    In read packet, pass in rawFormat = true

    Make sure everything still works in 006.

    Now, when writing packet prefix with one byte (0s)

    Next, when reading packet, read in one byte first.

    Might need an extra packet copy to do this. Shitty.

    Much better idea. "prefixBytes" passed in to PacketInfo, skip these bytes on write and read!!! (on write, fill with zeros, leave it up to caller to fill)

    On read, avoids shitty copy.

    OK. After a bunch of work, prefix bytes work with crc32. Wowwwwch.

    -------------

    Seems like I have more work to do, for example if I want the prefix bytes to be considered as part of the crc32
    this is going to suck ass.

    Seeing as where it really counts, I'm going to be using encryption instead of CRC32, fuck it, don't include prefix bytes in crc32.

    OK done.

    Next, setup the network interface to write in raw format if encrypted, and stick the packet flag into the prefix byte.

    Alright. Now we know if a packet is encrypted on not based on the flags in the first byte.

    Now rework the encryption so that it uses that first byte as the additional data. It needs to be signed!

    Moved the encryption into the yojimbo_crypto.h, all the stuff I need is in there.

    -------------

    Now implement the actual client/server setup as per the dedicated server, with the new connection packet.

    Stripped out unnecessary usage of client salt, challenge salt etc.

    In most cases client salt becomes client id.

    Need to ask friends if encoding short messages, eg. 0, 1, 2 bytes with the symmetric cipher weakens it. 

    Seems strange.

    -------------

    Now actually need to go through the motion with the client processing the new connection request

    eg. 

    1. create a token

    2. pass that token to the server in the connection request

    3. have the server verify the token in the connection request

    4. setup encrypted communications between the client and server, via address -> (nonce,key) mapping

    5. reply back with the challenge response

    6. client should have setup encryption already with the server via (nonce,key) it got back
       when requesting the token.

    Needs to be something that stops the client using the same token multiple times
    for different connections, eg. the token is only good for one IP address:port pair,
    the first one that uses it gets bound to that token, and the token won't work again
    for a different IP address to get a challenge response.

    Alternatively, put the client IP inside the token, that way the client cannot
    spoof multiple clients connecting with the same token, but *would* fuck up with NAT

    So blacklist on tokens already used and being bound to a particular address = SOLD

    -----
Tuesday May 17th, 2016
======================

    Find a way to pass context into read and write packet functions.

    PacketInfo struct passed into all read/write functions makes it easy to extend packet read/write in future.

    Also, makes sure both read and write have access to the packet factory, which will be required.

    Converted all example programs over to use the new stuff.

    Also made packet factory destroy packet a virtual, so it can be overridden to create packets from a custom heap.

    This should cover the protocol2 for embedding as a library as it does no other dynamic allocations.


Monday May 16th, 2016
=====================

    Implement rate limiting for various packets sent

    Implement client-side timeouts

    Implement client re-rolls the client salt if no response to connection request within one second.

    Implement challenge response send rate.

    Implement server-side timeout of a client

    Extend client states to include all error states

    Implement disconnect packets

    Server-side handle disconnect packets.

    Implement code on client that handles connection request denied, both reasons.

    Convert yojimbo.cpp stuff to use allocator

    Fix up stupid nested namespaces for array::something_whatever => array_something_whatever

    Implement counters for socket interface.


Sunday May 15th, 2016
=====================

    Bunch of planning and work implementing the client/server.

    Network interface setup and client/server logic for packet exchange working.


Saturday May 14th, 2016
=======================

    Now need to implement the network interface concept. protocol2.h?

    Do I need to implement send queues? Yes I think so. The data structures for these send queues belong in protocol2.h

    I should probably start getting a lot more of protocol2.h under functional test as well. eg. the send queues.

    Can't put the network interface in protocol2.h because it relies on network2::Address

    Can't put it in network2.h because it relies on protocol2::Packet and Read/WritePacket functions.

    Seems that we need to have a header file that unifies the two. eg. NetworkInterface.h?

    common.h? client.h? server.h?

    why not have all these together in the same file? *yojimbo.h* (beats "client_server.h")

    Yojimbo means bodyguard! This is awesome!

    Need to implement network interface for BSD sockets.

    Start here, and implement only this and get back to the client/server implementation.

    Keep it lean. Don't turn everything into a giant framework!

    Also, thinking a lot about hash tables, whatever hash table implementation you use, if
    the attacker uses that implementatino they can implement the worst case for you, by crafting
    nasty keys. It seems therefore that maybe it would even be preferrable to take worst case
    behavior on something simple, like a linear search instead.

    ---------

    Seems you can actually do better.

    What if the server has a cryptographically secure random number, and it never shares this number with clients,
    but hashes this number when it hashes whatever client data maps to the key, so in effect, clients cannot predict
    which connection requests correlate to which keys, negating attacks on hash table worst behavior with connection
    request packets.

    This is an important point to consider!

    -----------

    Added a bunch of extra logic for packet creates

    -----------

    Finish the actual client/server and setup a basic test bed with some simple tests showing connect, disconnect, reconnect, keep-alives, etc.

    Actual client connects now, looks good!


Friday May 13th, 2016
=====================

    Implement LAN version (unsecured) of client/server in code

    Move on to implement locked down secured version before it leaves my head!

    Implemented basic logic for server challenge.

    Don't want to go with a complicated hash delete. Annoying. Hard to test. Stupid waste of time. Easy to fuck up!

    How to make it robust on collisions?

    Idea: client rolls a new salt if it gets no response within 1 second?

    This will give it another slot, so if somebody else is using that slot, client is unblocked.

    This is a really smart approach, and easy to test. Keep it simple!

    Done! Sold. Easy.

    Now I really need to get a network interface going, some way to send and receive packets.

    In my previous implementation the actual serialization of the messages was performed later,
    eg. they were added to a queue.

    How about implementing a network interface implementation inside the example program first,
    and then seeing how it can be generalized into the library? (this is a good general pattern...)

    How about just getting the basic BSD sockets implementation into the network2.h?

    Halfway through this.

    Need to bring across the actual internel send and receive functions for the socket class.

    Then once this is done, can bring across an actual send queue or something like that.

    Key design goal here is to keep the protocol2 aspect of packets separate from the over the wire sending.

    This probably means that packets and send queues *and* serialization need to belong in protocol2

    While the actual gritty of sending that over the wire and receiving over the wire belongs in network2

    Brought across the rest of the socket class, eg. 

    Now I need to actually test the socket send/recv are working properly.

    After this, hook up a network interface (protocol layer) that handles send and receive queues
    and the stream and the context necessary for packet serialization.

    Cleaned it up and verified that socket send and receive packet are functional.


Thursday May 12th, 2016
=======================

    Seems like we have settled on a solution.

    1. Client connects to matchmaker over HTTPS, requests to be matched.

    2. Server and all dedis share a private key. Dedis must be hosted outside of player control, eg. data center.

    3. Matchmaker returns encrypted block of data with HMAC as signature using that private key to client.

    4. Client passes in this block of data and HMAC in the connection request packet.

    5. Server tests all connection request packets to see if valid via HMAC. If not valid, ignore and no response.

    6. If valid, test the hash of that data vs. previously sent blocks and if a block with the same hash has already been consumed, ignore the connection request and no response.

    7. Otherwise, send a challenge packet and assign a client slot to that particular connecting client (pending successful challenge response to avoid IP return address spoofing)

    8. Block of data must contain either the specific dedi ip or a whitelist of dedi IPs, to avoid attack where client takes the same block, runs a botnet and goes wide throwing connection requests with that block to every dedi in your data center (once)

    9. Block of data must also contain an expiry timestamp, this is useful because this timestamp can be used to expire old block entries in the blacklist.

    10. The block can also contain a randomly generated key for symmetric encryption so client and server can communicate securly.

    11. The matcher must also communicate the list of server IPs to try, and the key for communication with each dedi to the client.
        This is necessary because the client cannot decrypt the block, but it's OK because matcher sends this data to the client over HTTPS

    ------------

    To move forward with this solution it seems that:

        1. The client/server connection should be the climax of the series, not in the middle.

        2. I should implement a client/server connection for LAN, and then poke holes in it, how it can be attacked.

        3. Then I should implement a backend and secure it with crypto.

        4. I need to research HMAC

        5. I need to pick a backend language for the web service (node.js? python? ruby?)

        6. I need to pick a web server, nginx? Most likely.

        7. Should the web server for this game be the same one that serves my videos. Probably not.

        8. The final end result of this game should be an authoritative client/server networking of the cubes demo.

        9. This is verify definitely an "eat your own dogfood" type exercise.

        10. Should the authentication be done via facebook perhaps? Sign in with facebook. get a token to enter into the web service.

        11. I need to integrate a secure HTTPS web service into my game client.

    --------------


Wednesday May 11th, 2016
========================

    Talking with no bugs today about security. Seems that 

        a) I have a lot to learn

        b) the crypto challenge/response *is* important to delay botnets sending you connection requests

        c) the token idea is probably correct, but I'll have to run it through Shiring (done) and see what he says

    Basically, it seems that bugs does it all at the protocol layer (all in one) which makes sense for oldschool,
    but web tech seems to separate out the part, so I can see how he would not see this approach being valid.

    But I think the split between the two, with web doing what it does the best, and dedis doing just their thing is better.

    -------------

    Basic implementation idea

        1. Client sends connection request packet, it contains client id (globally unique uint64_t), client salt (random uint64_t, different one on each connection attempt),
           address:port of the client. Also, a block of data added at the end (eg. 256 bytes) to make it so connection requests cannot be used to DDOS amplify.

        2. Server looks at the client id, if the client id matches any clients which are currently connected to the server, the connection request is ignored.

           This is important, because otherwise, you could nuke somebody's connection by crafting a fake connection request from their IP that nukes them back to 
           starting a new connection *or* you can never have two connections coming from the same time from the same address:port combination.

           But is there any value to the client salt? Like at all? Bottom line, connection requests should be ignored while there is a current connection
           matching that address in the set of connected clients. (and I mean a match of both address and port)

        2. Server hashes the client id, client salt and the client address:port into a 64bit integer (murmurhash2)

        3. The modulo of this hash # is taken and it is the index into the challenge array

        4. The server checks the entry in the challenge array. 

                a) If the entry has a different address, client id or client salt, *and* the time the last challenge packet was sent is < ChallengeTimeOut seconds ago, 
                   the client connection request is ignored.

                b) If the entry has a different address, client id or client salt, *and* the time the last challenge packet was sent is >= ChallengeTimeOut seconds ago, 
                   the challenge entry is replaced with the new client address/id/salt and server queues up a challenge response to that packet.

                c) If the entry has a matching address, client id and client salt, but the time the last challenge packet was sent is < ChallengeResendTime seconds ago,
                   the client connection request is ignored.

                d) If the entry has a matching address, client id and client salt, *and* the time the last challenge packet was sent is >= ChallengeResendTime seconds ago,
                   the server queues up a challenge response packet to be sent to the client.

        5. At no point does the server ever allocate anything, do any work, or iterate over the set of pending challenges. Server only acts
           in response to a connection request packet sent to it, and not in a 1-1 response, except when a new connection attempt comes in, 
           after this the server only sends another packet in response to a client packet sent after a certain amount of time.

        6. The challenge packet sent to client contains the client id, client salt, and server salt generated for that challenge (random).
           This way the only way the client can respond correctly to the challenge, is if the client is a valid IP address, not a spoofed
           return address, and the client processed the challenge packet, read it, got the server salt out of it, and responded back.

        7. Does the client salt make it possible for one IP to initiate a bunch of connections on the same IP:port? Yes it does. This isn't cool. Why allow this? Stupid.

           But does this really matter? The attacker can trivially create different clients on ports and easily fill a server that way...

           The real protection only comes when there is a globally unique id that the server limits only one of those ids per-server via some sort of ticketing,
           eg. encrypted block of data, that allows the client to get past the connection request gate on the dedi.

    -------------------

    What is the point of the client salt? So that if another player knows the IP of another player, they cannot spoof that particular player 
    by forging packets with that player's IP address to try to disrupt connection. eg. whatever packets they send will likely have a different salt,
    and thus will not interfere with the challenge/response.

    The key is to both allow the salt, but also, to enforce only one client with a given client id on the server at a time.

    I think the salt still has value.

    -------------------

    Another key point, if there are packet types the server doesn't handle, the server needs to discard those packets at an early stage, vs. wasting resources,
    so I think there should be a bitfield for disabling packet types for receive on the packet factory level. That way we can have separate packet factories
    for the client and server, and they can be configured to discard packet types not expected to be received on each side, removing a vector for attack.

    -------------------

    Next, when connection payload packets come in check their address vs. currently connected clients, and check that the hash matches.

    Is it sufficient to just check the address? No, because spoofing allows other clients to make it looks like a post-connection packet is coming for that client.

    A lot of this stuff may not make any sense once you start encrypting... Think about this.

    -------------------

    It seems to me unless I am willing to setup the client id so that it comes from a valid source (I'm not),
    then there is very little point putting it in this protocol.

    So lets start with the BEST we can do on our own, a server, which if the IP is discovered, can be DDoS'd
    and potentially filled with zombie clients.

    identify its weaknesses and then follow up with a second implementation that addresses them, 
    maybe following up with a server backend or something like that with encryption.

    Awesome idea. Make a server auth system so you have to sign in with facebook in order to play?

    Hmmmmmmmm....

    It's really starting to seem like the client/server implementation should be at the end,
    not at the middle of the article series.

    eg. that I should build up to it with reliable packets, reliable time critical messages with blocks and so on,
    and then at the end, build up a naive client/server, a better client/server, and then finally a secure one with back end.
    (and an actual game at that!)

    This would be the best approach I think.

    Plus, I kinda need to do this, in order to be able to support my customers. I have to have experience doing these things.

    --------------------------

    There is a bug in the murmur2 hash implementation I have. It's not generating different hashes for strings with a different last character. Ouch!

    I broke it when I refactored it today. Idiot.

    ---------------

    If you know somebody elses IP address because of client salt, you can't hash collide with them. You don't know that salt.

    But you can deny new connections by spamming the server trivially with connection requests.

    I don't believe there is any way to stop this at the dedi level. Fundamentally, you have to let people attempt to connect.

    Also, if people do connect, well, they connect. If they are connecting maliciously, eg. one user is able to take up more
    resources on servers than they are entitled to, that is the real DDoS here from a dedicated server point of view.

    It therefore follows that no matter what is done on the dedicated server, it cannot protect against this.

    It seems another system for authentication, eg. to push it up to the web layer.

    ---------------

    At one suggestion from Jon, without ticketing, you can have a private key that the web auth knows, and the dedis know. Client does not know.

    The matchmaker can recieve a block from the client (over SSL), and send it back to the client encrypted (such that it is opaque to that client)

    The client then passes that block to the dedi in the connection request as a block of data.

    The server can use this to determine the client *came* from the matchmaker, eg. decrypt it, check for a string that matches, and then
    decode some data inside it like, uint64_t client_id, client name, etc.

    Maybe even potentially the client IP that the client should be connecting from, but this maybe excessive.

    Then this gives the client a secure way to communicate their connection request to the server.

    Since a DDoS attacker does not know the private key, they cannot create a connection request that passes the test and it gets ignored (no response).

    Also, since the clients should only be sent to a dedicated server from the matchmaker, which does rate limiting, DDoS should not be able to 
    mount a protocol level zombie client and/or DDoS attack on connection requests filling up that hash.

    In theory, this should allow the matchmaker to authenticate on that particular user id, and rate limit sessions that user can request through the matcher.

    If a particular client user id requests too many matchmaker requests in quick succession, they can be throttled.

    Also, through the matchmaker they can't really specify the *exact* server they want to land on, so any DDoS is distributed across multiple servers.

    ----------------

    Things to do to move forward:

    Research HMAC.

    Find a bcrypt implementation to use in C.

    Also need a high quality random number implementation for the salt.

    Need to pick a backend language.

    Need to work out what HTTPS library I use for my dedis and client to communicate with the matcher service.

    Implement a non-secure version of the client/server protocol as a starting point.

    This is important for analysis, to show how it can be attacked at a protocol level (not even really DDoS)

    ------------------

    Bonus: looks like "libsodium" should do the trick https://labs.opendns.com/2013/03/06/announcing-sodium-a-new-cryptographic-library/

    ------------------

    Key thing here that I am doing is known as this:

    https://en.wikipedia.org/wiki/Authenticated_encryption

    ------------------

    Looks like this library does exactly what I need, and provide a primitive for *exactly* what I want.

    http://www.daemonology.net/blog/2009-06-24-encrypt-then-mac.html

    ------------------

    Another option to consider, but I'm leaning towards lib sodium at this point.

    ------------------

    One more option to consider, perhaps as a replacement for OpenSSL for the web part:

    https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS

    ------------------

    This will be useful for the SSL certs

    ------------------



Tuesday May 10th, 2016
======================

Cut reliable packets over UDP

Going direct to the client/server structure first.

Some future notes for the chunk system:

    Implementation work todo on chunk sender:

        a) if the receiver stalls out in a chunk receive, it should count as a timeout (set a reasonable time for them to receive the chunk),
           otherwise the reciever can hang the protocol? or not. maybe for the blocks as part of the reliable message stream...

        b) burst mode

        c) bandwidth limiting

    These things should be done for the pro version at the end of the article series, when everything is brought together into one protocol.

    Work out how to sketch out the client/server

    I think I want to just do it straight up as an example program vs. a library initially.

    Once it's sketched out, think how it can be made in a library, perhaps with the last article in this program being an example.

    I'm going to start with the simplest form, which is to keep it separate.

    That way the client/server connection article can be focused on just that, and not on, for example, chunk slices or payload packets.

    Key points, 

    Make sure the client connect packet is much larger than the response (DDoS protection)

    The challenge -> response makes sure the return address on the packet actually matches the sender.

    Once the connection is established, use stream context to early reject packets that should have matching client/server guids pairs, but don't.

    (perhaps for the main server connection packet, hash the two 64bit integers into a 32bit hash)

    This avoids spoofing packets that cost a lot of time for you to process, if the client hasn't gone through the handshake process.

    Hard to guess a random 32bit number that will work, but I guess, they could try to brute force that. But how would they know? Vast majority won't get through.

    Next, is there any defense against a malicious botnet spawning connects?

    Seems that the one defense is to keep POTENTIAL connections separate from slots, eg. when a connection request comes in,
    put it in a separate hash of "INCOMING" eg: address and guid, but don't actually assign a slot yet. Of course, if all slots 
    are full, reject it.

    IMPORTANT: Don't actually walk over the incoming connections. Instead, when a packet comes in, reply immediately with the challenge,
    if the challenge is new, or it hasn't been responded to within x seconds. This way no linear scan over the incoming connections
    is ever required.

    What this does is require the person to actually have a valid address in the connection request packet, but if they actually
    do have a botnet, vs. one linux box spoofing addresses, they can still DDoS your server.

    It seems that there is little point to a server guid, because if you wanted to botnet you could just sniff the server guid
    once by sending a valid connection request with your own address, then once you have that server guid, just include it in
    all spoof packets you send off. So it creates just a small amount of safety, which would be cracked quickly, and no real
    protection.

    What adds some actual protection would be a unique salt created per-each incoming connection, so we know 

    What if somebody just DDoS lots of different ports from the same address? On one hand, this could be stopped
    by checking if too many connections have come from the same address from different ports, on the other hand,
    NAT issues could mean that lots of connections come through from the same effective IP address, on different ports.

    Look into how OpenTNL does the client challenge/response. How does it work and what exactly does it solve?

    If the challenge can be extended, then there needs to not be a timeout on the pending connections before they
    can be promoted on challenge response to an actual connection.

    This is another reason why the challenge/responses should most likely be a separate hash vs. the same # of slots.

    Going to need a hash table implementation.

    ... or do I?

    Does the hash table really need anything complicated like quadratic probing?

    What if instead the hash table was lossy, so it was overly large, but if a collision existed, that particular connection request would get whacked.

    But the two people connecting at the same time would FIGHT and this could be bad, because neither will be able to actually connect.

    It seems I do need a proper hash table.

    So what to do when the hash table is 50% full (capacity)?

    Reject additional connections until stuff comes through?

    Weird.

    No. A full hash table doesn't have the behavior that I want.

    What if instead, if a connection request comes in first, it sticks around for some period, like 30 seconds max,
    then if another person tries to connect with a hash collision they are just ignored, but then after the 30 seconds
    they will not be ignored, and will take over the slot, and *they* get the next 30 seconds.

    This would stop the fighting, and have better behavior if the hash table starts to get full, eg. collisions
    naturally resolve themselves vs. probing.

    This could work, and I think is much better behavior than I would get from a true closed hash table with probing.

    I think I must implement it like this, but make a note in the article that the naive way has problems.


Sunday May 1st, 2016
====================

    I'm feeling blocked on the article.

    I also feel that I need to do more research so I can write authoritatively rather than making stuff up.

    For example, I know that sending in a burst gets the data across faster on a LAN, but does it mess up over the internet?

    What about packet flooding sending all at once, vs. distributing the slice sends over time?

    What about a combined initial burst + the distributed slice sending. Does this actually get chunks across faster?

    Is there any difference between distributed slices sending and repeated bursts of unacked slices?

    What about the QoS idea of starting off fast and then backing off.

    Perhaps the article is better restructured around the implementation I already have:

        1. Sender
        2. Receiver
        3. Acks
        4. Blocks
        5. Burst optimization (show effectiveness)

    Yes, I think this is the best, and if I implement the burst and toggle it on/off and show that it's safe over a crappy connection,
    (do a bit of research here), it's a good article and I don't waste a bunch of time down dead ends and crazy ideas, or explaining
    and idea we're not really going to use.


Saturday April 23rd, 2016
=========================

    Went back over all previous examples and added soak behavior.

    Soaking is essential to guarantee correct behavior. Passes.

    Port to linux (linode box)

    Bunch of errors/warnings in there.

    All good now. Worthy exercise!

    Port to windows.

    How to get git going under win64?

    Need to also install premake for windows.

    Wow... there are a lot of issues...

    Bunch of problems with winsock...

    Ignoring winsock for now. Disabling network aspects...

    Fixing a bunch of compile errors, switch to alloca...

    002 example hangs... (WHY?!!!)

    Had to disable some warnings, simply no way to get around.

    Runtime errors can happen on truncation of int, eg. bits set in high. Crazy!

    Port back to MacOSX...

    Unix doesn't like #include <malloc.h> for alloca? Fixed. 

    Port back to linux... done.

    Back to windows... o_O

    Fixed issue in 002, it was a problem with too much resolution in random_float

    Windows version of premake needs to delete "protocol2.VC.db" on clean

    How to fix ld: warning: directory not found for option '-L/usr/lib64' on Mac?!

    ^--- worked around. Aint got nothing to do with me.

    Remove network2.h from projects that don't use it. 001, 002, 003.

    How to fix /arch:SSE2 on windows? Done.

    Try upgrading to premake5?

    It probably has better support for vs2015.
    
    It does, but clean is not ported yet, fucking lazy bastard...

    Also, it sticks the binaries in some stupid place so my build aliases don't work...

    FAAAAAAAAAAK...

    Fix this mess.

    Alright. Implemented my own clean.

    Moved output to "bin" so I could execute it.

    Verified working.

    Removed all the stuff that disabled network2.h socket stuff.

    Now to get that working again in VS15!

    Back to windows!

    Broken VS2015 install. Repaired.

    Rest of breakage was caused by #define min #define max fucking up the random_int and random_float functions


Friday April 22nd, 2016
=======================

    There is a bug in chunk sender when the chunk size is small (eg. 2-10 bytes or so)

    Reduced maximum chunk size to a point where it's failing 100%

    What the fuck is going on?!

    It's the end of packet serialize check...

    Why is it failing here and not elsewhere?!

    Ah.... it must be a desync that only shows up with small fragments?

    Mmmmm. I'm pretty sure it's a bug in serialize_bytes!

    Confirmed. If I replace the serialize_bytes with a simpler implementation, it works fine.

    There must be a bug when serialize_bytes has a small number of bytes (eg. no body)

    Fixed by using the simple non-head/tail codepath for read/write bytes if the # of bytes is <= 8.


Sunday April 3rd, 2016
======================


    Now actually need to hook up the example serialization code in 002 and make it as clean as possible.

    I think I'm going to skip the +/- integer trick, or put it before the object array stuff in the article?

    Sorted out randomized object data.

    Ready to finish writing article.
    

Saturday April 2nd, 2016
========================

    Make sure to actually test the serialize_bytes and serialize_string

    It's actually really annoying to have to use serialize methods when you are splitting read/write, eg. for sets of objects.

    Maybe split out and have:

        int value = read_bits( stream, 32 );

        write_bits( stream, EOF, 32 );

    etc...

    I think this is the best.

    Best of both world!

    Sketched out methods.

    Moved compressed float into 002 example. Not part of core library.


    It seems I should be able to get read int to work in-place nicely like this:

        uint32_t value = read_bits( stream, 32 );

    The problem is that I'm going to need to implement error checking in an exception like way, and I can't return false to indicate error.

    So... can this actually be done the way I want it to be done? I don't think it actually can be done.

    Frankly, this really sucks. If I can't solve this, move on.

    Found a better idiom:

        int value; serialize_bits( stream, value, 32 );

    Redefine the read/write functions to not work off Stream::IsReading etc.

    Seems to be good.

    Now rework the packet read/write functions to not be sucky with typedef Stream

    And cleanup the definition of the serialize function.


Saturday March 26th, 2016
=========================

    Need to sort out the endian issues and make sure my bitpacker lines up with what I described in my article.

    I may have fucked it up actually!

    Yes. It was reversed, eg. rolling bits the wrong way, required big endian to function.

    Reworked it to match the description in the article.

    Just need to finish updating ReadBytes/WriteBytes and the source code is available.

    Done. But it's not unit tested.

    Added unit test for serialize_bytes


Sunday January 17th, 2016
=========================

    Work out what the example should look like.

    I think it probably looks like:

    one side sends reliable packets to the other

    then the other side switches around and does the same thing

    (demonstrate bidirectionality)

    Started setting up the sender/receiver but it doesn't make sense this way because the acks must be bidirectional,
    and the aggregate packet header needs parts from the receiver (acks) while the including packets come from the sender

    Therefore created a "Node" class which handles both send and recieve.

    Sketched out all the functions I think that are required to get it working.

    Cut the article on implementing reliable messages inside a packet type, I think that is overkill for this series. Keep it simple.

    Now it will go from this reliability system to the client/server connection (implementing all these concepts together)

    At the end of this will be a useful bunch of source code, and two useful one file libraries that don't try to do too much.


Thursday January 7th, 2016
==========================

    Added error handling to read and write packet functions.

    Added code to check that read packets match written packets (they do)

    Need to think about the best interface to get the packet headers in/out of the functions.

    Right now current setup just doesn't work. Need array of pointers to packet headers.

    Alternatively, need to know the stride for the packet headers so we can move forward in memory to the next one from the current pointer.

    Array of pointers to the packet headers seems more *flexible* for the caller, because they can decide how they are laid out in memory.

    Pointer to start of array, and stride of array is the simplest interface to pass in, but a bit naughty.

    Decision: Went with protocol2::Object** pointer to array of pointers, because it is the most flexible.

    Add packet headers to the test

    Implemented header serialization. Works!

    Print sequence number out when printing packets that were read.

    Verify read packet header match written ones (same sequence #)

    Soak the example and make sure it is stable

    Ready to upload!

    Plan next article example source code:

    Next articles:

        1. Reliable Packets over UDP
        2. Time-Critical Reliable Messages over UDP
        3. Building a Better Ack System over UDP
        4. Reliable Messages with Bandwidth Limit

    Reliable Packets: The Plan

    Need to add a header for the aggregate packet. This is where ack data will go.

    Need a bit in header per-acket for reliable vs. unreliable packet in the aggregate packet.

    Do all the work for reliable and unreliable packets inside the example, as we won't do it this way moving forward.

    Separate sequence numbers for reliable and unreliable packets (otherwise, lots of unreliable packets in between reliable packets causes weirdness)

    Extended read/write aggregate packet function to take optional aggregate packet header.

    Rename 005 packet headers into two parts:

        1. aggregate packet header (acks will go here)

        2. packet header (sequence, reliable true/false)

    Implement the code that serializes the optional aggregate packet header, and aligns to byte.


Wednesday January 6th, 2016 (USA time)
=====================================

    Implement the main loop of 004 using the provided interfaces before actually implementing the functions.

    This will let me know if the interfaces make sense or suck.

    At minimum, going to need to adjust how the packet headers are passed in to read aggregate packet, and how read packets
    are returned from that function to caller.

    Implement write aggregate packet function

    Setup code to read the aggregate packet

    Read aggregate packet is mismatching on the crc32. What's going on?!

    Was just passing the wrong number of bytes to read. MaxPacketSize instead of bytesWritten. Fixed.

    Iterate across the read packets to make sure it's all good and works.

    No. There is a strange desync where bits are left over, and reading back packets seem to be occasionally 1 byte less than written.

    Probably some strange thing caused by writing packets piecemeal and copying over, vs. all at once in the stream, BUT...

    It's annoying, and needs to be fixed.

    The amount differing is actually small, and is on the bits processed level.

    For example writing packets, 64 bits becomes 63, 136 bits becomes 131 etc.

    I think it's alignment related.

    136 bits = 17 bytes

    131 bits = 16.375 bytes (rounded up to 17...)

    I think it's just an error on the serialize read align not increasing the bits processed correctly

    Yes. That was it.

    Now it is not quite lining up the exact number of bits so it has bits to spare on serialize read.

    Missing align after END packet marker => fixed.

    When the align fails, it is because there is an extra 8 bits left over.

    What is causing this?

    Fuck it. Who cares.


Wednesday January 6th, 2016 (on plane back to LA)
=================================================

    Next article should be about sending messages, or packet aggregation.

    I am concerned about the patent aggregation patent.

    Perhaps should be presented more like, OK we are sending messages in a packet type, "Connection" packet.

    Think this through.

    Decision: code it initially as packet aggregation.

    Talk to Mike Zyda. If the packet aggregation packet is still an issue, reframe it packing messages into a packet to work around (different implementation), as I normally do.

    Sketch out 004_packet_aggregation.cpp

    Now design a new packet format, *not compatible with one packet at a time format*

    Packet format:

        <implicit prefix header for crc32>
        [crc32]
        [packet type+1]
        [sequence]
            (packet data)
            (align byte)
        [packet type+1]
        [sequence]
            (packet data)
            (align byte)
        [nop 0]
        (end of packet sentinel)

    Design function interface for send packets, receive packets.

    Note, may return less than the full number of packets asked to be sent, in which case they need to be buffered.

    Question: Should I allow smaller unreliable packets to attempt to fit in after a large one that doesn't fit? 

    Possibly, but it seems advanced at this stage. Maybe want to punt on that.

    Especially seeing as I generally want to say, OK, you wanted to send x packets, but I could only send y.

    vs. for example a bitfield saying, OK I could actually send only the following packets.

    Decided. Will stop sending packets on the first one that doesn't fit in.

    Interface for sending packets function

    Inputs:

        0. protocol id
        1. num packets (>=1)
        2. maximum packet size that can be written

        per-packet:

            header protocol object pointer
            protocol2::Packet pointer
        
    Outputs:

        0. number of input packets that were written to the aggregate packet
        1. size of packet written
        2. the aggregate packet data

    Wrapped functions with #if PROTOCOL2_PACKET_AGGREGATION

    I want to make it easy to disable.

    It may be possible to retroactively make the format for "WritePacket" backwards compatible with read packets
    with a small change, that can be retroactively adjusted for the previous example programs so they are compatible.

    However, this would be painful to back port to previous examples, especially fragmentation and re-assembly.

    This idea is rejected.

    Sketched out both functions. Rather complicated.

    Some tricky stuff with array of references to pointers.

    For now, went with array of pointers to pointers (array).

    In reality this is really a pointer, to pointer to pointer of packets. This is where I'm not 100% certain what is the best idea.

    I definitely need to allocate the pointers from the factory as they are read in, so I can't actually accept pre-existing pointers
    to 

    Same thing exists for packet headers as well, except... I don't know how to create packet headers. I may require the user to pass
    in n valid pointers to packet header objects allocated by themselves. So I can then just serialize them in turn.

    Otherwise I'm going to need a factory for packet headers, which will suck.

    It is really smart to design the system so that zero packets can be written

    That way the empty header acts as a keep-alive, and without thinking you can pump out
    an aggregate packet, even if you don't have any real packets to send.

    Note that this is a property of the NOP design for packet aggregation.

    Every iteration generate up to n packets, including possibility of zero packets.

    Store these packets in an array of ptrs and keep track of number.

    Dummy stub out code to send the packets.

    Delete the array of packets

    Detect error on write aggregate packet and stop iterating.

    Fixed weirdness with WriteAggregatePacket prototype fn. not getting picked up

    
Tuesday January 5th, 2016
=========================

    Extend simulator to take address with packet send and receive.

    Need both to and from addresses per-packet queued in simulator. Added.

    Convert over the network simulator to work from raw packet data and size vs. packet structure

    Fix bugs in the network simulator causing it to loop forever when receiving packets.
    (was not clearing the entry on receive)

    Put random_int, random_float inside network2.h header. Needed for simulator.

    Update 003 example to have an address for sender and receiver (::1 with different port numbers)

    Implement function to send packet, eg. write it to data and then send it via simulator

    Loop to read packets from simulator. If packet is received, read it and then take the packet
    and work out from packet type how it should be processed, eg. slice packet -> receiver,
    ack packet -> sender.

    Seems to be some error in the simulator (eg. after 1024 entries).

    It's not wrapping around properly. Seems to stop receiving packets after it wraps around?

    Was a bug on the delivery time for packets on entries. Fixed.

    Also, there is something weird happening on packet read, it's triggering a read error "???"
    This means an error condition not handled. WTF is going on?!

    I think it's an uninitialized value, eg. a codepath in read packet that is not clearing to zero or setting the error code,
    but is passing it through, uninitialized value.

    Yes. Fixed.

    There is a bug that looks like truncated packets on read. The last serialize check is failing.

    This is the first time that I've actually passed in the bytes written on read? I think?

    If so this could be the source of the error. I'm not doing anything crazy.

    Yes. I was able to simulate this error in the 001 case by actually copying the packet data (memcpy)
    across and clearing the data and copying across just bytes written.

    So what is the error? Why didn't it initially trigger on 001 example but does now?

    I'll bet it's the byte order for the last uint32_t flushed, eg. it is storing the last byte as:

    [0][0][0][x]

    But only sending up to:

    [0]

    Fix is probably to switch around network byte order when writing to the buffer.

    ^---------- DO THIS

    Convert network order back to big endian.

    Didn't fix it. What's going on?!

    Seems to just be a byte order around the wrong way when reading 32 bits for check.

    Not sure why it would be wrong in this case, but pass unit tests for all other cases (?!)

    Yes. There was a bug where the protocol id was getting incorrectly bswapped, and then this
    bswapped protocol id was being compared vs. the packet one that was correct.

    This was benign when big endian was used as the network byte order, because the bswap
    became a noop.

    Serialize check is still failing for the ack and slice packet read/write. WTF

    Was incorrectly deleting packet data pointer before returning to caller.

    Implement latency.

    Implement packet loss.

    Make sure the 003 example runs correctly with simulator now (latency only)

    Of course now that packet loss is enabled, it is broken. Obviously some logic problem in the acks.

    Get 003 example running under packet loss.

    Fixed by unfucking stupid incorrect advance of currentSliceId

    Soak with packet loss works fine.

    Implement jitter.

    Implement duplicates.

    Verify the soak mode runs indefinitely with simulated out of order, packet loss, latency etc.

    Ready to upload to supporters! :D


Monday January 4th, 2016
========================

    Create network2.h

    Roughly port across network simulator to network2.h

    Port over network initialize and shutdown, as well as platform detection for socket headers etc.

    Add network address header

    Add network address implementation

    Bring across unit tests for network address


Sunday January 3rd, 2016
========================

    Implement send chunk method

    Implement send slice packet

    Implement process slice packet

    Implemented read packet and added concept of "ready to read" which blocks receiving slices of next chunk until caller reads the received chunk.

    Sketch out send ack packet (going to need a time for last ack sent, eg. once every 1/10th of a second)

    Implement process ack packet

    Cache the number of slices in the previous chunk on chunk receive.

    If a slice comes in from previous chunk, and the num slices for that chunk are not zero, force an ack for that chunk.

    Implement code that sends the forced ack for previous chunk.

    Implement code that sends ack for current received chunk.

    Setup packet send and receive loop

    Randomize the length of chunks and the data inside each chunk

    Make sure the send slices and acks work

    Add soak mode (-1) that loops forever.

    Track down random assert that is breaking the soak.

    Soak seems to break once chunk id hits 65535. Strange. Why?    

    Some random uint16_t -> int promotion weirdness on wrap.

    Add code to verify chunk size and chunk data contents recieved matches what was sent.

    Fixed a bunch of bugs in calculating the size of the last slice

    Example code must have a soak mode, eg. set num iterations to -1

    Verify soak runs for a significant time with perfect network conditions.

    Sketched out new interface for network simulator. Previous GDC 2015 network simulator too complicated for what I want to do.

    Added interface for packet loss, duplicates, jitter and latency.

    Sketched out the new network simulator interface.

    Implement a really basic network simulator that works without any packet loss, delay, dupes, etc.

    Cut duplicate packet support.

    
Saturday January 2nd, 2016
==========================

    Sketched out required packets for chunk slice sender/receiver

    Sketch out chunk sender

    Sketch out receiver struct


Friday December 25th, 2015
==========================

    Implementing the function to re-assemble and receive packets from the packet buffer.

    Example code is now fully functional. Time to clean up!

    Cleaned up and much nicer output.

    Wrote a merry xmas message to my patreon supporters and uploaded the latest example source code for paid supporters only.


Thursday December 24th, 2015
============================

    Mostly fleshed out new FragmentPacket structure with serialize function

    Finish implementation of fragment packet serialize, especially bit where fragmentSize is inferred.

    On read, make sure fragment size is clamped and checked to be in range, and if outside range we just
    abort the serialize vs. feeding into serialize_bytes.

    Converted code that writes packet fragment to use serialize write functions on stream.

    Removed "Fragment" struct and replaced with two arrays.

    Convert code that reads packet fragment to serialize read functions.

    I'm concerned that serialize read may not correctly work non 

    Yes it freaks out because of padding, and also if we pass in the padded amount as size, we cannot correctly
    infer the correct fragment size, because it has padded up to nearest dword and will be wrong.

    It seems the only correct way to handle this is to extend the read stream so that it can handle
    non-multiple of 4 bytes in the buffer, but it is safe to say that they actual underlying buffer
    should be a multiple of 4 bytes so the last dword does not read out of bounds memory.

    Convert read stream to support non-multiple of 4 buffer sizes.

    Clean up remaining protocol2.h to work with this.

    eg. remove hacks in read_packet that pad out to 4 bytes.

    Verify still works with 001 reading and writing packets example.

    Now 002 example is not passing because of something to do with read align. 

    What is going on?!

    Was a desync in the packet header. Fixed.

    Fixed how fragment header is parsed, and it now saves off the packet type in a struct var,
    so I can get the sequence # and crc32 out of the pre-parsed fragment packet for regular packets, 
    because I need to know which sequence number to store those packets as.

    Verify packet crc32 at the early stage before adding to the packet fragment buffer.

    Fixed a bunch of silly stuff where read packet was clearing the first 4 bytes of the packet (crc32)
    to zero in order to get the checksum to pass correctly. Applied the same fix to fragment data, which is
    to split the crc32 calculation into three separate parts, avoiding modifying a read-only packet buffer.


Wednesday December 23rd, 2015
=============================

    At this point it's starting to feel that integrating support for fragments into the protocol2.h is worth considering

    Here are the pros of that:

        1. It's done once and it works for all other articles past this
        2. It's reusable and exists for other examples without extra work
        3. There is very little overhead with the approach of reserving packet type 0 for fragments
        4. It's really annoying mirroring the packet read/write functions specially to handle fragments, it's also error prone

    Cons:

        1. It significantly complicates protocol2.h
        2. It turns protocol2.h from a stateless header into something that maintains a data structure
        3. The data structure has a lot of assumptions built in, like maximum number of fragments per-packet, max fragment size and so on.
        4. Baggage to support fragmentation and re-assembly is carried across to other articles that don't need it
        5. Now there are multiple versions of protocol2.h for each article *or* the protocol2.h gets really bulky, eg. lots of extra stuff
        6. I'm not really making middleware here, but a bunch of articles showing how stuff is done. It's good for each article to basically
           be self-contained. If somebody really wants the middleware for protocol2.h to be integrated together into a product they can
           pay me to do that for them.
        7. If I make this decision now, suddenly other things specific to articles will also get carried across to protocol2.h past this point
        8. I'd really like to keep protocol2.h as simple as possible and not turn it into a library.

    At the moment, I think I'm going to move forward with mirroring the packet read/write in the 002 article source code.


Tuesday December 8th, 2015
=========================

    Implement packet header object and pass it in to the 002_* example source code.

    Packet header just contains uint16_t sequence.

    Need a function to swap uint16_t to network, back to host order etc.

    Insert sequence into fragment packet

    Insert fragment id into fragment packet

    Insert total fragments into fragment packet

    Sketched out "ProcessPacket" function

    Hooked up so it gets called. Verified that it correctly distinguishes between fragment and non-fragmented packets.

    Added code to receive packets from packet buffer, vs. reading them directly off the packet write buffer.
    
    Parse the rest of the packet fragment header. Print out the results as procesing each packet, eg. x/y of packet n

    Parse the sequence number out of the regular packet too and print that.

    Pass to the process packet inside the packet buffer.

    Sequence number is not parsing correctly for non-fragmented packets. Is it getting written correctly?!

    I think there is a difference with what is written through the serialize vs. what is written if I read/write
    manually.

    It seems that I have to go through serialization always.

    Explore this.

    Write some code that determines what the byte order is for writing bytes at a time and shorts.

    eg. 0x10,0x11,0x12,0x13 for bytes

    or: 0x1111 and 0x2222 for shorts

    My guess is that I may have some code that switches the bit order of what I write.

    Confirmed. Serializing 0x11, 0x22, 0x33, 0x44 results in reading 0x33, 0x44 in the high word.

    So this means that writing one dword at a time is switching around the shorts.

    I'm pretty sure this means I need to go through the serializer all the time.
    

Sunday December 6th, 2015
=========================

    Sketch out data structure for packet buffer

    Implement advance function for packet buffer.

    Implement process fragment function for packet buffer.

    Sketch out function to split packet into fragments

    Sketch out code to process incoming packets and pass them to the packet buffer.

    Sketch out code to get packets from packet buffer in one loop

    Almost finished function to split packets apart into fragment packets

    Converted remaining comment pseudocode into code for the split packet function.

    Added "Header" protocol object which provides an in for the user creating their own header for packets.


November 30th, 2015
===================

    Make sure all serialize functions are getting called from test.cpp

    Add premake file for clean, make all etc. annoying to manually delete bins before checking in.

    Fix a bunch of bullshit with dev environment being broken by el capitan

    Get building with maximum warning levels on clang.

    Fix compiler warnings in release build (check macro needs to not be assert)

    Make sure it builds and works in release and debug builds.

    Clean up test to consolidate the context into the one test_stream function.


November 29th, 2015
===================

    Convert stream errors to PROTOCOL2_ERROR_* form and add to error_string fn.

    Fix GCC specific stuff (eg. __builtin_bswap etc)

    Make sure stream properly handles overflow on read. On write, stream should assert (it's your fault!)

    Split rest of existing template serialize fns to serialize_x_internal and wrap with macros for return false.

    Move the packet factory into implementation section.

    Make a list of all serialize functions to test.

        1. serialize_int
        2. serialize_bits
        3. serialize_bool
        4. serialize_float
        5. serialize_uint64
        6. serialize_double
        7. serialize_bytes
        8. serialize_string
        9. serialize_align
        10. serialize_check
        11. serialize_object


November 25th, 2015
===================

    Fixed weird serialization error.

    Make sure code doesn't blow up if only one packet is defined.

    Handle the protocol id in the checksum calculation, eg. pre-header

    Add code to get strings for various enums (especially error codes)

    Add comparison operators (==) to test packet types.

    Verify the packets received match the packet data sent.


October 21st, 2015
==================

    Implement unit tests

    Sketched out packet read/write for article 001

    Added concept of multiple errors: overflow, invalid, abort

    GetError() returns 0 if no error (PROTOCOL2_STREAM_ERROR_NONE)

    Make SerializeInteger return true/false on success/failure again

    Update macros to return false on failure

    Make sure there is not double checking for overflow (stream and bit reader etc.)

    Find public domain CRC32 implementation

    Clean up the CRC impl.

    Write functions to calculate packet CRC

    Move read and write packet into protocol2.h (generally useful)

    Split apart protocol2.h into implementation and header, STB style.

    Add enum for packet read error. Packet write error can only have one cause (serialize write failure), and you can already check it on the stream.

    Add more interesting serialization examples to make up the packets, eg. an array of things, a string, some byte data, position/velocity with at rest flag.

    Randomize the data sent in the packets

    Add prints showing the numbers of bytes in packets

    Added serialize_float (uncompressed)
