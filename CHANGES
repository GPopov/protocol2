
Sunday January 17th, 2016
=========================

    Work out what the example should look like.

    I think it probably looks like:

    one side sends reliable packets to the other

    then the other side switches around and does the same thing

    (demonstrate bidirectionality)

    Started setting up the sender/receiver but it doesn't make sense this way because the acks must be bidirectional,
    and the aggregate packet header needs parts from the receiver (acks) while the including packets come from the sender

    Therefore created a "Node" class which handles both send and recieve.

    Sketched out all the functions I think that are required to get it working.

    Cut the article on implementing reliable messages inside a packet type, I think that is overkill for this series. Keep it simple.

    Now it will go from this reliability system to the client/server connection (implementing all these concepts together)

    At the end of this will be a useful bunch of source code, and two useful one file libraries that don't try to do too much.


Thursday January 7th, 2016
==========================

    Added error handling to read and write packet functions.

    Added code to check that read packets match written packets (they do)

    Need to think about the best interface to get the packet headers in/out of the functions.

    Right now current setup just doesn't work. Need array of pointers to packet headers.

    Alternatively, need to know the stride for the packet headers so we can move forward in memory to the next one from the current pointer.

    Array of pointers to the packet headers seems more *flexible* for the caller, because they can decide how they are laid out in memory.

    Pointer to start of array, and stride of array is the simplest interface to pass in, but a bit naughty.

    Decision: Went with protocol2::Object** pointer to array of pointers, because it is the most flexible.

    Add packet headers to the test

    Implemented header serialization. Works!

    Print sequence number out when printing packets that were read.

    Verify read packet header match written ones (same sequence #)

    Soak the example and make sure it is stable

    Ready to upload!

    Plan next article example source code:

    Next articles:

        1. Reliable Packets over UDP
        2. Time-Critical Reliable Messages over UDP
        3. Building a Better Ack System over UDP
        4. Reliable Messages with Bandwidth Limit

    Reliable Packets: The Plan

    Need to add a header for the aggregate packet. This is where ack data will go.

    Need a bit in header per-acket for reliable vs. unreliable packet in the aggregate packet.

    Do all the work for reliable and unreliable packets inside the example, as we won't do it this way moving forward.

    Separate sequence numbers for reliable and unreliable packets (otherwise, lots of unreliable packets in between reliable packets causes weirdness)

    Extended read/write aggregate packet function to take optional aggregate packet header.

    Rename 005 packet headers into two parts:

        1. aggregate packet header (acks will go here)

        2. packet header (sequence, reliable true/false)

    Implement the code that serializes the optional aggregate packet header, and aligns to byte.


Wednesday January 6th, 2016 (USA time)
=====================================

    Implement the main loop of 004 using the provided interfaces before actually implementing the functions.

    This will let me know if the interfaces make sense or suck.

    At minimum, going to need to adjust how the packet headers are passed in to read aggregate packet, and how read packets
    are returned from that function to caller.

    Implement write aggregate packet function

    Setup code to read the aggregate packet

    Read aggregate packet is mismatching on the crc32. What's going on?!

    Was just passing the wrong number of bytes to read. MaxPacketSize instead of bytesWritten. Fixed.

    Iterate across the read packets to make sure it's all good and works.

    No. There is a strange desync where bits are left over, and reading back packets seem to be occasionally 1 byte less than written.

    Probably some strange thing caused by writing packets piecemeal and copying over, vs. all at once in the stream, BUT...

    It's annoying, and needs to be fixed.

    The amount differing is actually small, and is on the bits processed level.

    For example writing packets, 64 bits becomes 63, 136 bits becomes 131 etc.

    I think it's alignment related.

    136 bits = 17 bytes

    131 bits = 16.375 bytes (rounded up to 17...)

    I think it's just an error on the serialize read align not increasing the bits processed correctly

    Yes. That was it.

    Now it is not quite lining up the exact number of bits so it has bits to spare on serialize read.

    Missing align after END packet marker => fixed.

    When the align fails, it is because there is an extra 8 bits left over.

    What is causing this?

    Fuck it. Who cares.


Wednesday January 6th, 2016 (on plane back to LA)
=================================================

    Next article should be about sending messages, or packet aggregation.

    I am concerned about the patent aggregation patent.

    Perhaps should be presented more like, OK we are sending messages in a packet type, "Connection" packet.

    Think this through.

    Decision: code it initially as packet aggregation.

    Talk to Mike Zyda. If the packet aggregation packet is still an issue, reframe it packing messages into a packet to work around (different implementation), as I normally do.

    Sketch out 004_packet_aggregation.cpp

    Now design a new packet format, *not compatible with one packet at a time format*

    Packet format:

        <implicit prefix header for crc32>
        [crc32]
        [packet type+1]
        [sequence]
            (packet data)
            (align byte)
        [packet type+1]
        [sequence]
            (packet data)
            (align byte)
        [nop 0]
        (end of packet sentinel)

    Design function interface for send packets, receive packets.

    Note, may return less than the full number of packets asked to be sent, in which case they need to be buffered.

    Question: Should I allow smaller unreliable packets to attempt to fit in after a large one that doesn't fit? 

    Possibly, but it seems advanced at this stage. Maybe want to punt on that.

    Especially seeing as I generally want to say, OK, you wanted to send x packets, but I could only send y.

    vs. for example a bitfield saying, OK I could actually send only the following packets.

    Decided. Will stop sending packets on the first one that doesn't fit in.

    Interface for sending packets function

    Inputs:

        0. protocol id
        1. num packets (>=1)
        2. maximum packet size that can be written

        per-packet:

            header protocol object pointer
            protocol2::Packet pointer
        
    Outputs:

        0. number of input packets that were written to the aggregate packet
        1. size of packet written
        2. the aggregate packet data

    Wrapped functions with #if PROTOCOL2_PACKET_AGGREGATION

    I want to make it easy to disable.

    It may be possible to retroactively make the format for "WritePacket" backwards compatible with read packets
    with a small change, that can be retroactively adjusted for the previous example programs so they are compatible.

    However, this would be painful to back port to previous examples, especially fragmentation and re-assembly.

    This idea is rejected.

    Sketched out both functions. Rather complicated.

    Some tricky stuff with array of references to pointers.

    For now, went with array of pointers to pointers (array).

    In reality this is really a pointer, to pointer to pointer of packets. This is where I'm not 100% certain what is the best idea.

    I definitely need to allocate the pointers from the factory as they are read in, so I can't actually accept pre-existing pointers
    to 

    Same thing exists for packet headers as well, except... I don't know how to create packet headers. I may require the user to pass
    in n valid pointers to packet header objects allocated by themselves. So I can then just serialize them in turn.

    Otherwise I'm going to need a factory for packet headers, which will suck.

    It is really smart to design the system so that zero packets can be written

    That way the empty header acts as a keep-alive, and without thinking you can pump out
    an aggregate packet, even if you don't have any real packets to send.

    Note that this is a property of the NOP design for packet aggregation.

    Every iteration generate up to n packets, including possibility of zero packets.

    Store these packets in an array of ptrs and keep track of number.

    Dummy stub out code to send the packets.

    Delete the array of packets

    Detect error on write aggregate packet and stop iterating.

    Fixed weirdness with WriteAggregatePacket prototype fn. not getting picked up

    
Tuesday January 5th, 2016
=========================

    Extend simulator to take address with packet send and receive.

    Need both to and from addresses per-packet queued in simulator. Added.

    Convert over the network simulator to work from raw packet data and size vs. packet structure

    Fix bugs in the network simulator causing it to loop forever when receiving packets.
    (was not clearing the entry on receive)

    Put random_int, random_float inside network2.h header. Needed for simulator.

    Update 003 example to have an address for sender and receiver (::1 with different port numbers)

    Implement function to send packet, eg. write it to data and then send it via simulator

    Loop to read packets from simulator. If packet is received, read it and then take the packet
    and work out from packet type how it should be processed, eg. slice packet -> receiver,
    ack packet -> sender.

    Seems to be some error in the simulator (eg. after 1024 entries).

    It's not wrapping around properly. Seems to stop receiving packets after it wraps around?

    Was a bug on the delivery time for packets on entries. Fixed.

    Also, there is something weird happening on packet read, it's triggering a read error "???"
    This means an error condition not handled. WTF is going on?!

    I think it's an uninitialized value, eg. a codepath in read packet that is not clearing to zero or setting the error code,
    but is passing it through, uninitialized value.

    Yes. Fixed.

    There is a bug that looks like truncated packets on read. The last serialize check is failing.

    This is the first time that I've actually passed in the bytes written on read? I think?

    If so this could be the source of the error. I'm not doing anything crazy.

    Yes. I was able to simulate this error in the 001 case by actually copying the packet data (memcpy)
    across and clearing the data and copying across just bytes written.

    So what is the error? Why didn't it initially trigger on 001 example but does now?

    I'll bet it's the byte order for the last uint32_t flushed, eg. it is storing the last byte as:

    [0][0][0][x]

    But only sending up to:

    [0]

    Fix is probably to switch around network byte order when writing to the buffer.

    ^---------- DO THIS

    Convert network order back to big endian.

    Didn't fix it. What's going on?!

    Seems to just be a byte order around the wrong way when reading 32 bits for check.

    Not sure why it would be wrong in this case, but pass unit tests for all other cases (?!)

    Yes. There was a bug where the protocol id was getting incorrectly bswapped, and then this
    bswapped protocol id was being compared vs. the packet one that was correct.

    This was benign when big endian was used as the network byte order, because the bswap
    became a noop.

    Serialize check is still failing for the ack and slice packet read/write. WTF

    Was incorrectly deleting packet data pointer before returning to caller.

    Implement latency.

    Implement packet loss.

    Make sure the 003 example runs correctly with simulator now (latency only)

    Of course now that packet loss is enabled, it is broken. Obviously some logic problem in the acks.

    Get 003 example running under packet loss.

    Fixed by unfucking stupid incorrect advance of currentSliceId

    Soak with packet loss works fine.

    Implement jitter.

    Implement duplicates.

    Verify the soak mode runs indefinitely with simulated out of order, packet loss, latency etc.

    Ready to upload to supporters! :D


Monday January 4th, 2016
========================

    Create network2.h

    Roughly port across network simulator to network2.h

    Port over network initialize and shutdown, as well as platform detection for socket headers etc.

    Add network address header

    Add network address implementation

    Bring across unit tests for network address


Sunday January 3rd, 2016
========================

    Implement send chunk method

    Implement send slice packet

    Implement process slice packet

    Implemented read packet and added concept of "ready to read" which blocks receiving slices of next chunk until caller reads the received chunk.

    Sketch out send ack packet (going to need a time for last ack sent, eg. once every 1/10th of a second)

    Implement process ack packet

    Cache the number of slices in the previous chunk on chunk receive.

    If a slice comes in from previous chunk, and the num slices for that chunk are not zero, force an ack for that chunk.

    Implement code that sends the forced ack for previous chunk.

    Implement code that sends ack for current received chunk.

    Setup packet send and receive loop

    Randomize the length of chunks and the data inside each chunk

    Make sure the send slices and acks work

    Add soak mode (-1) that loops forever.

    Track down random assert that is breaking the soak.

    Soak seems to break once chunk id hits 65535. Strange. Why?    

    Some random uint16_t -> int promotion weirdness on wrap.

    Add code to verify chunk size and chunk data contents recieved matches what was sent.

    Fixed a bunch of bugs in calculating the size of the last slice

    Example code must have a soak mode, eg. set num iterations to -1

    Verify soak runs for a significant time with perfect network conditions.

    Sketched out new interface for network simulator. Previous GDC 2015 network simulator too complicated for what I want to do.

    Added interface for packet loss, duplicates, jitter and latency.

    Sketched out the new network simulator interface.

    Implement a really basic network simulator that works without any packet loss, delay, dupes, etc.

    Cut duplicate packet support.

    
Saturday January 2nd, 2016
==========================

    Sketched out required packets for chunk slice sender/receiver

    Sketch out chunk sender

    Sketch out receiver struct


Friday December 25th, 2015
==========================

    Implementing the function to re-assemble and receive packets from the packet buffer.

    Example code is now fully functional. Time to clean up!

    Cleaned up and much nicer output.

    Wrote a merry xmas message to my patreon supporters and uploaded the latest example source code for paid supporters only.


Thursday December 24th, 2015
============================

    Mostly fleshed out new FragmentPacket structure with serialize function

    Finish implementation of fragment packet serialize, especially bit where fragmentSize is inferred.

    On read, make sure fragment size is clamped and checked to be in range, and if outside range we just
    abort the serialize vs. feeding into serialize_bytes.

    Converted code that writes packet fragment to use serialize write functions on stream.

    Removed "Fragment" struct and replaced with two arrays.

    Convert code that reads packet fragment to serialize read functions.

    I'm concerned that serialize read may not correctly work non 

    Yes it freaks out because of padding, and also if we pass in the padded amount as size, we cannot correctly
    infer the correct fragment size, because it has padded up to nearest dword and will be wrong.

    It seems the only correct way to handle this is to extend the read stream so that it can handle
    non-multiple of 4 bytes in the buffer, but it is safe to say that they actual underlying buffer
    should be a multiple of 4 bytes so the last dword does not read out of bounds memory.

    Convert read stream to support non-multiple of 4 buffer sizes.

    Clean up remaining protocol2.h to work with this.

    eg. remove hacks in read_packet that pad out to 4 bytes.

    Verify still works with 001 reading and writing packets example.

    Now 002 example is not passing because of something to do with read align. 

    What is going on?!

    Was a desync in the packet header. Fixed.

    Fixed how fragment header is parsed, and it now saves off the packet type in a struct var,
    so I can get the sequence # and crc32 out of the pre-parsed fragment packet for regular packets, 
    because I need to know which sequence number to store those packets as.

    Verify packet crc32 at the early stage before adding to the packet fragment buffer.

    Fixed a bunch of silly stuff where read packet was clearing the first 4 bytes of the packet (crc32)
    to zero in order to get the checksum to pass correctly. Applied the same fix to fragment data, which is
    to split the crc32 calculation into three separate parts, avoiding modifying a read-only packet buffer.


Wednesday December 23rd, 2015
=============================

    At this point it's starting to feel that integrating support for fragments into the protocol2.h is worth considering

    Here are the pros of that:

        1. It's done once and it works for all other articles past this
        2. It's reusable and exists for other examples without extra work
        3. There is very little overhead with the approach of reserving packet type 0 for fragments
        4. It's really annoying mirroring the packet read/write functions specially to handle fragments, it's also error prone

    Cons:

        1. It significantly complicates protocol2.h
        2. It turns protocol2.h from a stateless header into something that maintains a data structure
        3. The data structure has a lot of assumptions built in, like maximum number of fragments per-packet, max fragment size and so on.
        4. Baggage to support fragmentation and re-assembly is carried across to other articles that don't need it
        5. Now there are multiple versions of protocol2.h for each article *or* the protocol2.h gets really bulky, eg. lots of extra stuff
        6. I'm not really making middleware here, but a bunch of articles showing how stuff is done. It's good for each article to basically
           be self-contained. If somebody really wants the middleware for protocol2.h to be integrated together into a product they can
           pay me to do that for them.
        7. If I make this decision now, suddenly other things specific to articles will also get carried across to protocol2.h past this point
        8. I'd really like to keep protocol2.h as simple as possible and not turn it into a library.

    At the moment, I think I'm going to move forward with mirroring the packet read/write in the 002 article source code.


Tuesday December 8th, 2015
=========================

    Implement packet header object and pass it in to the 002_* example source code.

    Packet header just contains uint16_t sequence.

    Need a function to swap uint16_t to network, back to host order etc.

    Insert sequence into fragment packet

    Insert fragment id into fragment packet

    Insert total fragments into fragment packet

    Sketched out "ProcessPacket" function

    Hooked up so it gets called. Verified that it correctly distinguishes between fragment and non-fragmented packets.

    Added code to receive packets from packet buffer, vs. reading them directly off the packet write buffer.
    
    Parse the rest of the packet fragment header. Print out the results as procesing each packet, eg. x/y of packet n

    Parse the sequence number out of the regular packet too and print that.

    Pass to the process packet inside the packet buffer.

    Sequence number is not parsing correctly for non-fragmented packets. Is it getting written correctly?!

    I think there is a difference with what is written through the serialize vs. what is written if I read/write
    manually.

    It seems that I have to go through serialization always.

    Explore this.

    Write some code that determines what the byte order is for writing bytes at a time and shorts.

    eg. 0x10,0x11,0x12,0x13 for bytes

    or: 0x1111 and 0x2222 for shorts

    My guess is that I may have some code that switches the bit order of what I write.

    Confirmed. Serializing 0x11, 0x22, 0x33, 0x44 results in reading 0x33, 0x44 in the high word.

    So this means that writing one dword at a time is switching around the shorts.

    I'm pretty sure this means I need to go through the serializer all the time.
    

Sunday December 6th, 2015
=========================

    Sketch out data structure for packet buffer

    Implement advance function for packet buffer.

    Implement process fragment function for packet buffer.

    Sketch out function to split packet into fragments

    Sketch out code to process incoming packets and pass them to the packet buffer.

    Sketch out code to get packets from packet buffer in one loop

    Almost finished function to split packets apart into fragment packets

    Converted remaining comment pseudocode into code for the split packet function.

    Added "Header" protocol object which provides an in for the user creating their own header for packets.


November 30th, 2015
===================

    Make sure all serialize functions are getting called from test.cpp

    Add premake file for clean, make all etc. annoying to manually delete bins before checking in.

    Fix a bunch of bullshit with dev environment being broken by el capitan

    Get building with maximum warning levels on clang.

    Fix compiler warnings in release build (check macro needs to not be assert)

    Make sure it builds and works in release and debug builds.

    Clean up test to consolidate the context into the one test_stream function.


November 29th, 2015
===================

    Convert stream errors to PROTOCOL2_ERROR_* form and add to error_string fn.

    Fix GCC specific stuff (eg. __builtin_bswap etc)

    Make sure stream properly handles overflow on read. On write, stream should assert (it's your fault!)

    Split rest of existing template serialize fns to serialize_x_internal and wrap with macros for return false.

    Move the packet factory into implementation section.

    Make a list of all serialize functions to test.

        1. serialize_int
        2. serialize_bits
        3. serialize_bool
        4. serialize_float
        5. serialize_uint64
        6. serialize_double
        7. serialize_bytes
        8. serialize_string
        9. serialize_align
        10. serialize_check
        11. serialize_object


November 25th, 2015
===================

    Fixed weird serialization error.

    Make sure code doesn't blow up if only one packet is defined.

    Handle the protocol id in the checksum calculation, eg. pre-header

    Add code to get strings for various enums (especially error codes)

    Add comparison operators (==) to test packet types.

    Verify the packets received match the packet data sent.


October 21st, 2015
==================

    Implement unit tests

    Sketched out packet read/write for article 001

    Added concept of multiple errors: overflow, invalid, abort

    GetError() returns 0 if no error (PROTOCOL2_STREAM_ERROR_NONE)

    Make SerializeInteger return true/false on success/failure again

    Update macros to return false on failure

    Make sure there is not double checking for overflow (stream and bit reader etc.)

    Find public domain CRC32 implementation

    Clean up the CRC impl.

    Write functions to calculate packet CRC

    Move read and write packet into protocol2.h (generally useful)

    Split apart protocol2.h into implementation and header, STB style.

    Add enum for packet read error. Packet write error can only have one cause (serialize write failure), and you can already check it on the stream.

    Add more interesting serialization examples to make up the packets, eg. an array of things, a string, some byte data, position/velocity with at rest flag.

    Randomize the data sent in the packets

    Add prints showing the numbers of bytes in packets

    Added serialize_float (uncompressed)
